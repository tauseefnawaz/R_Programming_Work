Accuracy = res$byClass["Pos Pred Value"]
print("Accuracy: ")
print(Accuracy)
library(caTools)
library(ggplot2)
data <- read.table("student-mat.xls",header=1,sep = ';')
sum(is.na(data))
columns = names(data)
####################visualizing the dataset ##########
#plotting school column
ggplot(data, aes(x = school)) + geom_bar(fill="blue", position="dodge")
#plotting sex column
ggplot(data, aes(x = sex)) + geom_bar(fill="blue", position="dodge")
#plotting Address column
ggplot(data, aes(x = address)) + geom_bar(fill="blue", position="dodge")
#plotting fsize column
ggplot(data, aes(x = famsize)) + geom_bar(fill="blue", position="dodge")
#plotting Mjob column
ggplot(data, aes(x = Mjob)) + geom_bar(fill="blue", position="dodge")
#plotting guardian column
ggplot(data, aes(x = guardian)) + geom_bar(fill="blue", position="dodge")
#plotting schoolsup column
ggplot(data, aes(x = schoolsup)) + geom_bar(fill="blue", position="dodge")
#As schoolsup has mostly "no" value, so we don't need in dataframe, it may cause overfitting because of not equally distributed
data = data[,-(15:16)]
#plotting famsup column
ggplot(data, aes(x = famsup)) + geom_bar(fill="blue", position="dodge")
#plotting nursery column
ggplot(data, aes(x = nursery)) + geom_bar(fill="blue", position="dodge")
#plotting romantic column
ggplot(data, aes(x = romantic)) + geom_bar(fill="blue", position="dodge")
data <- transform(
data,
school = as.integer(factor(school)),
sex = as.integer(factor(sex)),
address = as.integer(factor(address)),
famsize = as.integer(factor(famsize)),
Pstatus = as.integer(factor(Pstatus)),
Medu = as.integer(factor(Medu)),
Fedu = as.integer(factor(Fedu)),
Mjob = as.integer(factor(Mjob)),
Fjob = as.integer(factor(Fjob)),
reason = as.integer(factor(reason)),
guardian = as.integer(factor(guardian)),
famsup = as.integer(factor(famsup)),
paid = as.integer(factor(paid)),
activities = as.integer(factor(activities)),
nursery = as.integer(factor(nursery)),
higher = as.integer(factor(higher)),
internet = as.integer(factor(internet)),
romantic = as.integer(factor(romantic))
)
sapply(data, class)
data = data[,-(29:30)]
data[,c(1:12)] = scale(data[,c(1:12)])
#train test split
split = sample.split(data$G3,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
#random forest
library(randomForest)
rf <- randomForest(
G3 ~ .,
data=training
)
pred = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
#random forest
library(randomForest)
rf <- randomForest(
G3 ~ .,
data=training
)
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
actual = test[:c(1:28)]
actual = test[,c(1:28)]
actual = test[,c(1:28)]
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
sapply(data, class)
data = data[,-(29:30)]
data[,c(1:28)] = scale(data[,c(1:28)])
#train test split
split = sample.split(data$G3,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
data <- read.table("student-mat.xls",header=1,sep = ';')
sum(is.na(data))
columns = names(data)
#plotting school column
ggplot(data, aes(x = school)) + geom_bar(fill="blue", position="dodge")
#plotting sex column
ggplot(data, aes(x = sex)) + geom_bar(fill="blue", position="dodge")
#plotting Address column
ggplot(data, aes(x = address)) + geom_bar(fill="blue", position="dodge")
#plotting fsize column
ggplot(data, aes(x = famsize)) + geom_bar(fill="blue", position="dodge")
#plotting Mjob column
ggplot(data, aes(x = Mjob)) + geom_bar(fill="blue", position="dodge")
#plotting guardian column
ggplot(data, aes(x = guardian)) + geom_bar(fill="blue", position="dodge")
#plotting schoolsup column
ggplot(data, aes(x = schoolsup)) + geom_bar(fill="blue", position="dodge")
#As schoolsup has mostly "no" value, so we don't need in dataframe, it may cause overfitting because of not equally distributed
data = data[,-(15:16)]
#plotting famsup column
ggplot(data, aes(x = famsup)) + geom_bar(fill="blue", position="dodge")
#plotting nursery column
ggplot(data, aes(x = nursery)) + geom_bar(fill="blue", position="dodge")
#plotting romantic column
ggplot(data, aes(x = romantic)) + geom_bar(fill="blue", position="dodge")
data <- transform(
data,
school = as.integer(factor(school)),
sex = as.integer(factor(sex)),
address = as.integer(factor(address)),
famsize = as.integer(factor(famsize)),
Pstatus = as.integer(factor(Pstatus)),
Medu = as.integer(factor(Medu)),
Fedu = as.integer(factor(Fedu)),
Mjob = as.integer(factor(Mjob)),
Fjob = as.integer(factor(Fjob)),
reason = as.integer(factor(reason)),
guardian = as.integer(factor(guardian)),
famsup = as.integer(factor(famsup)),
paid = as.integer(factor(paid)),
activities = as.integer(factor(activities)),
nursery = as.integer(factor(nursery)),
higher = as.integer(factor(higher)),
internet = as.integer(factor(internet)),
romantic = as.integer(factor(romantic))
)
sapply(data, class)
data = data[,-(29:30)]
data[,c(1:28)] = scale(data[,c(1:28)])
split = sample.split(data$G3,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
library(randomForest)
rf <- randomForest(
G3 ~ .,
data=training
)
actual = test[,c(1:28)]
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
actual = test[,29]
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
linearMod <- lm(G3 ~., data = training)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
preds = predict(model1, newdata=test[,c(1:28)])
actual = test$G3
#Finding RSquare
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print(rsq)
print("Linear Regression R square :")
print(rsq)
##Implement with k-fold cross validataion
library(lattice)
library(DAAG)
cvResults <- suppressWarnings(CVlm(data, form.lm= G3 ~., m=5, dots=FALSE, seed=29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."))
attr(cvResults, 'ms')  # => 210.6192 mean squared error
library(caTools)
library(ggplot2)
data <- read.table("student-mat.xls",header=1,sep = ';')
sum(is.na(data))
columns = names(data)
####################visualizing the dataset ##########
#plotting school column
ggplot(data, aes(x = school)) + geom_bar(fill="blue", position="dodge")
#plotting sex column
ggplot(data, aes(x = sex)) + geom_bar(fill="blue", position="dodge")
#plotting Address column
ggplot(data, aes(x = address)) + geom_bar(fill="blue", position="dodge")
#plotting fsize column
ggplot(data, aes(x = famsize)) + geom_bar(fill="blue", position="dodge")
#plotting Mjob column
ggplot(data, aes(x = Mjob)) + geom_bar(fill="blue", position="dodge")
#plotting guardian column
ggplot(data, aes(x = guardian)) + geom_bar(fill="blue", position="dodge")
#plotting schoolsup column
ggplot(data, aes(x = schoolsup)) + geom_bar(fill="blue", position="dodge")
#As schoolsup has mostly "no" value, so we don't need in dataframe, it may cause overfitting because of not equally distributed
data = data[,-(15:16)]
#plotting famsup column
ggplot(data, aes(x = famsup)) + geom_bar(fill="blue", position="dodge")
#plotting nursery column
ggplot(data, aes(x = nursery)) + geom_bar(fill="blue", position="dodge")
#plotting romantic column
ggplot(data, aes(x = romantic)) + geom_bar(fill="blue", position="dodge")
data <- transform(
data,
school = as.integer(factor(school)),
sex = as.integer(factor(sex)),
address = as.integer(factor(address)),
famsize = as.integer(factor(famsize)),
Pstatus = as.integer(factor(Pstatus)),
Medu = as.integer(factor(Medu)),
Fedu = as.integer(factor(Fedu)),
Mjob = as.integer(factor(Mjob)),
Fjob = as.integer(factor(Fjob)),
reason = as.integer(factor(reason)),
guardian = as.integer(factor(guardian)),
famsup = as.integer(factor(famsup)),
paid = as.integer(factor(paid)),
activities = as.integer(factor(activities)),
nursery = as.integer(factor(nursery)),
higher = as.integer(factor(higher)),
internet = as.integer(factor(internet)),
romantic = as.integer(factor(romantic))
)
sapply(data, class)
data = data[,-(29:30)]
data[,c(1:28)] = scale(data[,c(1:28)])
#train test split
split = sample.split(data$G3,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
#random forest
library(randomForest)
rf <- randomForest(
G3 ~ .,
data=training
)
actual = test[,29]
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
#Linear Regression
linearMod <- lm(G3 ~., data = training)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
#############Model Evaluation###################
preds = predict(model1, newdata=test[,c(1:28)])
actual = test$G3
#Finding RSquare
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print(rsq)
print("Linear Regression R square :")
print(rsq)
library(lattice)
library(DAAG)
cvResults <- suppressWarnings(CVlm(data, form.lm= G3 ~., m=5, dots=FALSE, seed=29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."))
attr(cvResults, 'ms')  # => 210.6192 mean squared error
library(caTools)
library(ggplot2)
data <- read.table("bank.xls",header=1,sep = ';')
sum(is.na(data))
####################visualizing the dataset ##########
#plotting job column
ggplot(data, aes(x = job)) + geom_bar(fill="blue", position="dodge")
#plotting marital column
ggplot(data, aes(x = marital)) + geom_bar(fill="blue", position="dodge")
#plotting education column
ggplot(data, aes(x = education)) + geom_bar(fill="blue", position="dodge")
#plotting default column
ggplot(data, aes(x = default)) + geom_bar(fill="blue", position="dodge")
#As default has mostly "no" value, so we don't need in dataframe, it may cause overfitting because of not equally distributed
data = data[,-(4:5)]
#plotting housing column
ggplot(data, aes(x = housing)) + geom_bar(fill="blue", position="dodge")
#plotting loan column
ggplot(data, aes(x = loan)) + geom_bar(fill="blue", position="dodge")
#plotting contact column
ggplot(data, aes(x = contact)) + geom_bar(fill="blue", position="dodge")
#plotting month column
ggplot(data, aes(x = month)) + geom_bar(fill="blue", position="dodge")
#plotting poutcome column
ggplot(data, aes(x = poutcome)) + geom_bar(fill="blue", position="dodge")
data = data[,-(13:14)]
#for pdays column
ggplot(data, aes(x = pdays)) + geom_histogram(fill="blue", position="dodge",binwidth=100)
data$pdays[data$pdays==-1] = mean(data$pdays)
#for balance column
data$balance[data$balance==0] = mean(data$balance)
#########################Data preprocessing###############
data <- transform(
data,
age = age,
job = as.integer(factor(job)),
marital = as.integer(factor(marital)),
balance = balance,
housing = as.integer(factor(housing)),
loan = as.integer(factor(loan)),
contact = as.integer(factor(contact)),
day = day,
month = as.integer(factor(month)),
duration = duration,
campaign = campaign,
pdays = pdays
)
sapply(data, class)
#train test split
split = sample.split(data$y,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
training[,c(1:12)] = scale(training[,c(1:12)])
#applying the model
#install.packages("randomForest")
library(randomForest)
rf <- randomForest(
y ~ .,
data=training
)
test[,c(1:12)] = scale(test[,c(1:12)])
pred = predict(rf, newdata=test[,c(1:12)])
cm = table(test[,13], pred)
fourfoldplot(cm, color = c("#CC6666", "#99CC99"),conf.level = 0, margin = 1, main = "Confusion Matrix")
library(caret)
res = confusionMatrix(cm)
print(res)
Accuracy = res$byClass["Pos Pred Value"]
print("Accuracy: ")
print(Accuracy)
library(randomForest)
rf <- randomForest(
y ~ .,
data=training
)
test[,c(1:12)] = scale(test[,c(1:12)])
pred = predict(rf, newdata=test[,c(1:12)])
cm = table(test[,13], pred)
fourfoldplot(cm, color = c("#CC6666", "#99CC99"),conf.level = 0, margin = 1, main = "Confusion Matrix")
library(caret)
res = confusionMatrix(cm)
print(res)
Accuracy = res$byClass["Pos Pred Value"]
print("Accuracy: ")
print(Accuracy)
library(caTools)
library(ggplot2)
data <- read.table("bank.xls",header=1,sep = ';')
sum(is.na(data))
####################visualizing the dataset ##########
#plotting job column
ggplot(data, aes(x = job)) + geom_bar(fill="blue", position="dodge")
#plotting marital column
ggplot(data, aes(x = marital)) + geom_bar(fill="blue", position="dodge")
#plotting education column
ggplot(data, aes(x = education)) + geom_bar(fill="blue", position="dodge")
#plotting default column
ggplot(data, aes(x = default)) + geom_bar(fill="blue", position="dodge")
#As default has mostly "no" value, so we don't need in dataframe, it may cause overfitting because of not equally distributed
data = data[,-(4:5)]
#plotting housing column
ggplot(data, aes(x = housing)) + geom_bar(fill="blue", position="dodge")
#plotting loan column
ggplot(data, aes(x = loan)) + geom_bar(fill="blue", position="dodge")
#plotting contact column
ggplot(data, aes(x = contact)) + geom_bar(fill="blue", position="dodge")
#plotting month column
ggplot(data, aes(x = month)) + geom_bar(fill="blue", position="dodge")
#plotting poutcome column
ggplot(data, aes(x = poutcome)) + geom_bar(fill="blue", position="dodge")
data = data[,-(13:14)]
#for pdays column
ggplot(data, aes(x = pdays)) + geom_histogram(fill="blue", position="dodge",binwidth=100)
data$pdays[data$pdays==-1] = mean(data$pdays)
#for balance column
data$balance[data$balance==0] = mean(data$balance)
#########################Data preprocessing###############
data <- transform(
data,
age = age,
job = as.integer(factor(job)),
marital = as.integer(factor(marital)),
balance = balance,
housing = as.integer(factor(housing)),
loan = as.integer(factor(loan)),
contact = as.integer(factor(contact)),
day = day,
month = as.integer(factor(month)),
duration = duration,
campaign = campaign,
pdays = pdays
)
sapply(data, class)
#train test split
split = sample.split(data$y,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
#scalling the data
training[,c(1:12)] = scale(training[,c(1:12)])
library(randomForest)
rf <- randomForest(
y ~ .,
data=training
)
test[,c(1:12)] = scale(test[,c(1:12)])
pred = predict(rf, newdata=test[,c(1:12)])
cm = table(test[,13], pred)
fourfoldplot(cm, color = c("#CC6666", "#99CC99"),conf.level = 0, margin = 1, main = "Confusion Matrix")
library(caret)
res = confusionMatrix(cm)
print(res)
Accuracy = res$byClass["Pos Pred Value"]
print("Accuracy: ")
print(Accuracy)
linearMod <- lm(G3 ~., data = training)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
summary(linearMod)
library(caTools)
library(ggplot2)
data <- read.table("student-mat.xls",header=1,sep = ';')
sum(is.na(data))
columns = names(data)
####################visualizing the dataset ##########
#plotting school column
ggplot(data, aes(x = school)) + geom_bar(fill="blue", position="dodge")
#plotting sex column
ggplot(data, aes(x = sex)) + geom_bar(fill="blue", position="dodge")
#plotting Address column
ggplot(data, aes(x = address)) + geom_bar(fill="blue", position="dodge")
#plotting fsize column
ggplot(data, aes(x = famsize)) + geom_bar(fill="blue", position="dodge")
#plotting Mjob column
ggplot(data, aes(x = Mjob)) + geom_bar(fill="blue", position="dodge")
#plotting guardian column
ggplot(data, aes(x = guardian)) + geom_bar(fill="blue", position="dodge")
#plotting schoolsup column
ggplot(data, aes(x = schoolsup)) + geom_bar(fill="blue", position="dodge")
#As schoolsup has mostly "no" value, so we don't need in dataframe, it may cause overfitting because of not equally distributed
data = data[,-(15:16)]
#plotting famsup column
ggplot(data, aes(x = famsup)) + geom_bar(fill="blue", position="dodge")
#plotting nursery column
ggplot(data, aes(x = nursery)) + geom_bar(fill="blue", position="dodge")
#plotting romantic column
ggplot(data, aes(x = romantic)) + geom_bar(fill="blue", position="dodge")
data <- transform(
data,
school = as.integer(factor(school)),
sex = as.integer(factor(sex)),
address = as.integer(factor(address)),
famsize = as.integer(factor(famsize)),
Pstatus = as.integer(factor(Pstatus)),
Medu = as.integer(factor(Medu)),
Fedu = as.integer(factor(Fedu)),
Mjob = as.integer(factor(Mjob)),
Fjob = as.integer(factor(Fjob)),
reason = as.integer(factor(reason)),
guardian = as.integer(factor(guardian)),
famsup = as.integer(factor(famsup)),
paid = as.integer(factor(paid)),
activities = as.integer(factor(activities)),
nursery = as.integer(factor(nursery)),
higher = as.integer(factor(higher)),
internet = as.integer(factor(internet)),
romantic = as.integer(factor(romantic))
)
sapply(data, class)
data = data[,-(29:30)]
data[,c(1:28)] = scale(data[,c(1:28)])
#train test split
split = sample.split(data$G3,SplitRatio=0.8)
training = subset(data,split==TRUE)
test = subset(data,split==FALSE)
#random forest
library(randomForest)
rf <- randomForest(
G3 ~ .,
data=training
)
actual = test[,29]
preds = predict(rf, newdata=test[,c(1:28)])
rss <- sum((preds - actual) ^ 2)  ## residual sum of squares
tss <- sum((actual - mean(actual)) ^ 2)  ## total sum of squares
rsq <- 1 - rss/tss
print("Random Forest R square :")
print(rsq)
#Linear Regression
linearMod <- lm(G3 ~., data = training)
modelSummary <- summary(linearMod)  # capture model summary as an object
modelCoeffs <- modelSummary$coefficients  # model coefficients
summary(linearMod)
